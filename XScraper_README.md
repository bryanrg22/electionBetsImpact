# XScraper

Highâ€‘throughput Playwrightâ€‘powered **Twitter / X** scraper built for largeâ€‘scale
historical and realâ€‘time data collection.  
It is battleâ€‘tested on electionâ€‘cycle analysis but the modular design lets you
swap in any keyword set or HTML mapping without touching core logic.

---

## âœ¨Â Key Features

| Capability | Details |
|------------|---------|
| **Headless browser scraping** | Uses PlaywrightÂ 1.44 to authenticate with real session cookies and intercept GraphQL traffic directly â€“ no fragile HTML parsing needed. |
| **Keywordâ€‘window search** | JSON parameter decks let you define complex ORâ€‘chained search strings plus precise `since:` / `until:` timestamps down to the second. |
| **Rateâ€‘limit aware rotation** | Multiple authenticated accounts are loaded from cookie jars and rotated automatically whenever `x-rateâ€‘limitâ€‘remaining` is low. |
| **Incremental backâ€‘scroll** | Parser returns the epoch of the last tweet fetched; the scroller resumes from there, ensuring *zero overlap & zero gaps*. |
| **Pluggable parsers** | `helpers/parser_helpers.py` normalises SearchTimeline payloads; alternative mappers (BeautifulSoup / lxml) are supported via `mappers/*.json`. |
| **Crashâ€‘proof runs** | Unknown payload shapes are dumped to `errorfile.json` and the run continues, so long jobs never die midâ€‘stream. |
| **Unitâ€‘test fixtures** | Golden sample `data_preprocessed.test.json` plus a CSV smokeâ€‘test runner (`appCSVTest.py`). |

---

## ğŸ—‚Â Project Layout

```text
XSCRAPER/
â”œâ”€â”€ __pycache__/                    # compiledâ€‘bytecode cache
â”‚   â””â”€â”€ accounts_cookies*.pyc
â”œâ”€â”€ cookies/                        # exported Playwright session cookies
â”‚   â””â”€â”€ <username>.json
â”œâ”€â”€ helpers/
â”‚   â””â”€â”€ parser_helpers.py           # shared HTML / JSON parsing utilities
â”œâ”€â”€ mappers/
â”‚   â””â”€â”€ easy_scraper.json           # fieldâ€‘mapping template for tweet entities
â”œâ”€â”€ parameters/                     # searchâ€‘filter & runtime configs
â”‚   â””â”€â”€ <keyword_set>.json
â”œâ”€â”€ parsed_tweets/                  # cached, cleaned tweet payloads
â”‚   â”œâ”€â”€ _ipynb_checkpoints/
â”‚   â””â”€â”€ week_may30_june7_without_space_keywords_new_1.temp.json
â”œâ”€â”€ .DS_Store
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                       # quickâ€‘start & usage docs
â”œâ”€â”€ account_cookies.sample.py       # helper to save Twitter cookies in Playwright
â”œâ”€â”€ add_accounts.py                 # CLI tool â€“ add multiple handles / rotate logins
â”œâ”€â”€ app.py                          # entry point â€“ orchestrates scraping & CSV export
â”œâ”€â”€ appCSVTest.py                   # smokeâ€‘test runner for CSV output
â”œâ”€â”€ app.log                         # runtime logs (gitâ€‘tracked for demo)
â”œâ”€â”€ data_preprocessed.test.json     # example of final structured tweet data
â”œâ”€â”€ errorfile.json                  # sample error tracebacks for debugging
â”œâ”€â”€ requirements.txt                # pinned Python dependencies
â”œâ”€â”€ sample.json                     # tiny fixture tweet batch
â”œâ”€â”€ screenshot.png                  # UI snapshot for README
â””â”€â”€ screenshot_after10.png          # postâ€‘run validation screenshot
```


---

## ğŸ”§Â Setup

```bash
# 1ï¸âƒ£Â clone repo & create venv
git clone https://github.com/yourâ€‘handle/XScraper.git
cd XScraper
python -m venv .venv && source .venv/bin/activate

# 2ï¸âƒ£Â install deps
pip install --upgrade pip
pip install -r requirements.txt
playwright install  # installs browsers

# 3ï¸âƒ£Â add cookies
cp account_cookies.sample.py accounts_cookies.py
#   â†³  paste your auth_token, ct0, _twitter_sess for **each** account
```

> **Tip:** try the oneâ€‘liner in `appCSVTest.py` to autoâ€‘generate fresh cookie
> files if youâ€™re able to log in interactively.

---

## ğŸš€Â QuickÂ StartÂ (Production JSON run)

```bash
python app.py --parameters parameters/parameters(1).json               --output parsed_tweets/election_may30.json
```

*Arguments (also flagâ€‘parseable via ENV variables)*

| Flag | Description |
|------|-------------|
| `--parameters` | Path to the JSON deck defining keywords & date window. |
| `--output` | (optional) Override default output filepath. |
| `--headless` | `true` / `false` (default `true`). |

The scraper will:

1. Randomly choose a cookie jar, load it into a new Playwright context.
2. Build the advanced search string from the parameter deck.
3. Scroll the **Latest** feed while intercepting `/SearchTimeline` GraphQL calls.
4. Pipe each response through `helpers/parser_helpers.py`.
5. Append deâ€‘duplicated clean rows into `parsed_tweets/*.json`.

Progress, rateâ€‘limit hits, and errors stream to STDOUT &Â `app.log`.

---

## âš™ï¸Â Parameter File Anatomy

```jsonc
{
  "keywords": [
    "Joe Biden",
    "Donald Trump",
    "#Election2024"
  ],
  "startDate": "2023-05-30T00:00:00Z",
  "endDate":   "2023-05-31T00:00:00Z",
  "mode": "EXACT"
}
```

*Modify â†’ save â†’ reâ€‘run* to scrape a new slice without touching code.

---

## ğŸ—ï¸Â Parser Logic

![Parser flowchart](screenshot.png)

1. `parse_search_timeline_response()` traverses every `TimelineAddEntries`.
2. Each entryâ€™s raw tweet is flattened by `basic_tweet_content()`.
3. The helper injects convenience columns (`epoch`, booleans, URL) and returns a dict.
4. Results aggregate until chunk sizeÂ â‰¥Â 100 then flush to disk.
5. Unknown schema â†’ dump raw blob into `errorfile.json` with a timestamp.

See **helpers/parser_helpers.py** for the exact field list.

---

## ğŸ“ˆÂ Postâ€‘Processing

```python
import pandas as pd, json
df = pd.json_normalize(json.load(open(
    "parsed_tweets/week_may30_june7_without_space_keywords_new_1.temp.json")))
df.info()
```

Typical cleanâ€‘up steps:

* Drop placeholder `"PW"` columns â†’ `df.replace("PW", pd.NA, inplace=True)`
* Convert `epoch`Â â†’Â `pd.to_datetime(df["epoch"], unit="s")`
* Explode `links` and `mentionedUsers` arrays for network analysis.

---

## ğŸ›‚Â Rateâ€‘Limit & Account Rotation

* Twitter currently allows **300** requests / 15â€‘min window per loggedâ€‘in user.
* `app.py` watches the `x-rate-limit-remaining` header and sleeps whenÂ <â€¯20.
* When an account hits two consecutive coolâ€‘downs it hotâ€‘swaps to the next jar.
* Total tweets scraped are tracked per session; see the `--max-tweets` flag.

---

## ğŸªµÂ Logging

Enable full logs by exporting:

```bash
export XS_LOG=1
```

Youâ€™ll get an `app.log` file like:

```
2025â€‘06â€‘09 18:41 INFO  Selected account: @data_bot
2025â€‘06â€‘09 18:41 INFO  Scroll epoch start = 1717286400
â€¦
```

---

## ğŸ§°Â Troubleshooting

| Symptom | Fix |
|---------|-----|
| Browser opens but no tweets are captured | Verify your cookies are still valid (auth_token & ct0), run `appCSVTest.py` interactively. |
| Lots of entries in `errorfile.json` | X changed GraphQL schema â†’ update `parser_helpers.py` (see comments at top of file). |
| Playwright exits withÂ `ERR_CONNECTION_RESET` | Twitter is rateâ€‘throttling IP â€“ add VPN or wait 15Â min. |

---

## ğŸ—ºï¸Â Roadmap

* âœ¨Â **FUZZY** search mode (keyword stemming & typos)
* ğŸŒÂ HTTPâ€‘only fallback using `twscrape` for headlessâ€‘less servers
* ğŸ“ŠÂ Builtâ€‘in sqlite writer for instant SQL querying
* ğŸ§ªÂ CI unit tests on push (GitHub Actions)

---

## More In-Depth File Explanations
| File / folder                    | Purpose & key implementation points                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **helpers/**parser_helpers.py  | All of the heavy lifting for *turning raw GraphQL timeline JSON into analysisâ€‘ready records*.  <br>â€¢Â basic_tweet_content() normalises a single Tweet (handles three GraphQL payload variants) â†’ returns a flat dict of 30â€‘ish scalar fields + lists.<br>â€¢Â Adds computed helpers such as epoch (UNIX ts) and booleans like retweetedTweet so downstream code can filter easily.<br>â€¢Â parse_tweet_contents() figures out whether an entry is a single tweet (tweetâ€‘*) or a profile thread (profileâ€‘conversationâ€‘*) and delegates accordingly.  It logs the full offending payload to **errorfile.json** when it hits an unseen structure, which is why the scraper almost never crashes midâ€‘run.<br>â€¢Â parse_search_timeline_response() is tailorâ€‘made for the GraphQL SearchTimeline endpoint.  It pulls every *TimelineAddEntries* block, aggregates the cleaned tweets, and returns {data, tweet_count, last_tweet_epoch} â€” the last value drives the incremental â€œwalk backwards in timeâ€ logic in **app.py**.<br>â€¢Â parse_response() is an older helper for nonâ€‘search timelines (kept for completeness).                                                                                                                                                                                                                                                                                                                                         |
| **app.py**                       | **Main production scraper** (JSON output). Run it and it will:  <br>1. Load a *parameter set* (PARAMETERS_FILENAME) that defines **keywords**, start/end dates, and the maximum tweet age window.<br>2. Pick a random cookie jar from accounts_cookies.py (imported as **cookies**) so each session looks like a different loggedâ€‘in browser.<br>3. Spin up Playwright â†’ open *x.com* â†’ inject cookies (works around UI login).<br>4. Construct an advanced search string with get_search_string() (keywords wrapped in parentheses + until: & since: gates).<br>5. Scroll the *Latest* tab via keep_scrolling() while attaching page.on("response", intercept_response); that hook captures every GraphQL response, forwards it to parse_search_timeline_response(), and appends the cleaned rows to **parsed\_tweets/\*.json** using append_json_to_file_after_pre_processing() (deâ€‘dupes on tweet ID).<br>6. Tracks Xâ€™s rateâ€‘limit headers (x-rate-limit-remaining, x-rate-limit-reset). When **remaining** drops under a random threshold it time.sleep() until reset, then continues from the *last seen tweet epoch* so you never miss data.<br>7. Loops through multiple accounts (list_accounts) until the total_tweets_of_this_session passes the perâ€‘run quota you set in the parameter JSON.<br>*Outputs*:  A growing temp dump (**week\_may30\_june7\_without\_space\_keywords\_new\_1.temp.json**) ready for postâ€‘processing. |
| **appCSVTest.py**                | Sandâ€‘box / smokeâ€‘test variant of app.py that proves the pipeline endâ€‘toâ€‘end without hitting your prod JSON file.  Differences:<br>â€¢Â Imports **account\_cookies.sample.py** (you patch in one cookie set at a time).<br>â€¢Â Writes a **CSV** instead of JSON so you can validate in Excel quickly.<br>â€¢Â Has a relogin_to_update_cookies() helper that can *log in for real*, scrape the browser storage, and build fresh cookie jars in bulk (handy when Twitter invalidates tokens).<br>â€¢Â Contains a *multiprocessing* block (commented) showing how to shard scraping across CPU cores if you need more throughput.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| **account\_cookies.sample.py**   | Redâ€‘acted template showing the exact dict structure Playwright expects (name / value / domain / path / expires).  Use it as a blueprint to paste in real auth\_token / ct0 combos.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **accounts\_cookies1.py**        | Example of a *fully populated* cookie list (auth\_token, ct0, \_twitter\_sess, etc.).  app.py is currently importing accounts_cookies, so rename / symlink this file to **accounts\_cookies.py** (or edit the import) when you go live.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| **add\_accounts.py**             | Legacy utility (now fully commentedâ€‘out) that once bulkâ€‘loaded login creds into **twscrape**â€™s SQLiteâ€‘backed accounts.db.  Keep it for reference if you decide to switch from Playwright to an HTTPâ€‘only scraper.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| **mappers/**easy_scraper.json  | Declarative CSS/attribute mapping for an *alt* extraction path (beautifulâ€‘soap / lxml).  Lists selectors for avatar, username, embedded images, plus stat keys (replyCount, repostCount, â€¦).  Not invoked in the current Playwright flow but useful if Twitterâ€™s HTML layout changes and you want a quick swap.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| **parsed\_tweets/** (folder)     | Rolling cache of every cleaned tweet payload your runs have produced.  Naming convention is week_<start>_<end>_<desc>.json.  Each file is appended atomically so you can pipe them straight into pandas.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| **data\_preprocessed.test.json** | Tiny **golden sample** created by parser_helpers.test_json() to verify that your parsing logic hasnâ€™t regressed.  Great for unit tests.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| **errorfile.json**               | When parser_helpers hits an unexpected tweet structure it dumps the raw entry here so you can inspect and update the parser rules.  Keeps your main run from dying midâ€‘crawl.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **app.log**                      | Optional runtime log captured during long scrapes; not included here but, if enabled, youâ€™ll see timestamped INFO/ERROR lines from every major step in app.py.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| **requirements.txt**             | Pin list for Playwright (playwright==1.x), pandas / numpy, and any CLI helpers.  (File not shared yetâ€”see â€œnext filesâ€ below.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ****pycache**/**                 | Autoâ€‘generated *.pyc* byteâ€‘code files. Safe to ignore / add to .gitignore.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| **parameters/**parameters(1)â€¦parameters(5).json                                         | <u>ğŸ“Œâ€¯Purpose</u>â€¯â€“â€¯each JSON block is an *input deck* that app.py consumes at startup to define a single scraping pass.<br><br>**Key fields**  <br>â€¢Â keywords â€“ list of 50â€‘ish electionâ€‘related strings and hashtags (Biden, MAGA, RFKâ€¯Jr, etc.). These are <br>Â Â â€¢ joined with **OR** when mode:"EXACT" (current setting) â†’ the search string looks like (â€œJoeâ€¯Bidenâ€) OR (â€œDonaldâ€¯Trumpâ€) â€¦  <br>Â Â â€¢ wrapped by () and attached to Playwrightâ€™s /SearchTimeline GraphQL call.<br>â€¢Â startDate / endDate â€“ granular down to seconds; app.py converts to ISO (since:YYYYâ€‘MMâ€‘DD_HH:MM:SS_UTC) and until: gates so you can slide the window dayâ€‘byâ€‘day without code edits.<br>â€¢Â mode â€“ currently only "EXACT" supported, but parser\_helpers is written so you can add "FUZZY" later (would build a query without quotes and allow stemming).<br><br>**Why five versions?**Â You split a long JulyÂ 27Â â†’Â AugÂ 1 scrape into 24â€‘hour chunks to avoid Twitter rate limits and make reruns idempotent. app.py chooses the file you pass on the CLI and names the output cache accordingly. |
| **week_may30_june7_without_space_keywords_new_1.temp.json**                             | A *live run artifact* sitting in **parsed\_tweets/** (copied here for reference). The structure mirrors the list returned by parse_search_timeline_response() â€“ each element is a fully flattened tweet record with:<br>â€¢Â metadata (id, url, epoch, language flagsâ€¦)<br>â€¢Â engagement metrics (replyCount, viewCount etc.)<br>â€¢Â nested mentionedUsers / links arrays already normalised.<br><br>Fields marked "PW" are placeholders that parser_helpers.py swaps in when Twitter omits that attribute. When you later postâ€‘process the dataset you can safely treat "PW" as NULL. Good practice: drop this file into a Jupyter notebook and run pd.json_normalize() to explore the columns you actually need.                                                                                                                                                                                                                                                                                                                                                                             |
| **requirements.txt**                                                                    | The *reproducible environment lockfile* â€“ 32 pinned wheels.<br>Highlight stops:<br>â€¢Â playwright==1.44.0 & pyee â€“ headless browser driver + async event hooks used in app.py.<br>â€¢Â twscrapeÂ &Â ntscraper â€“ HTTPâ€‘only alternatives; you keep them around in case Twitter rateâ€‘limits Playwrightâ€”nice futureâ€‘proofing.<br>â€¢Â beautifulsoup4, lxml â€“ not in use by default flow, but required if you switch to the HTMLâ€‘scraper path driven by **mappers/easy\_scraper.json**.<br>â€¢Â pandas, numpy, greenlet â€“ data wrangling; greenlet speeds up Playwrightâ€™s cooperative multitasking.<br>Install withÂ pythonÂ -mÂ pipÂ installÂ -rÂ requirements.txt inside a fresh venv (PythonÂ 3.11+ recommended for PlaywrightÂ 1.44).                                                                                                                                                                                                                                                                                                                                                                          |
| **account\_cookies.sample.py** *(updated note)*                                           | Still the templateâ€”no secrets inside. Shows you the full dict schema (domain, expires, httpOnly, etc.) plus the wrapper list named cookies. Copy this to **accounts\_cookies.py**, paste your real auth_token, ct0, _twitter_sess, and youâ€™re ready to scrape multiple sessions in a roundâ€‘robin.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| **accounts_cookies.py** *(the real one you provided earlier as accounts_cookies1.py)* | Import target for app.py. It definesÂ ACCOUNTSâ€¯=Â [{username,Â cookies:[â€¦]}, â€¦] so Playwright can call context.add_cookies() and hop between logins whenever the rateâ€‘limit headers get tight.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |

---

## ğŸ“œÂ License

MIT â€“ feel free to remix, tweak, and build on top.  
See `LICENSE` for full text.

---

*Generated automatically on 2025â€‘06â€‘09 by ChatGPT.*